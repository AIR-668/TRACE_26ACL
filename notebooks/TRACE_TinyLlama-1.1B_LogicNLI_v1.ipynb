{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f495b5a-c432-42a0-8041-9f2778f2afc8",
   "metadata": {},
   "source": [
    "# ğŸ§± Cell 1ï¼šç¯å¢ƒä¸è·¯å¾„è®¾ç½®ï¼ˆGPU6ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc4d8d1-de32-4c60-8642-07aed758acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL\n",
      "CUDA_VISIBLE_DEVICES: 6\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: ç¯å¢ƒä¸è·¯å¾„è®¾ç½®ï¼ˆGPU6ï¼‰\n",
    "\n",
    "import os\n",
    "\n",
    "# åªä½¿ç”¨ GPU6\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ï¼ˆæŒ‰ä½ å®é™…è·¯å¾„æ¥ï¼‰\n",
    "PROJECT_ROOT = \"/beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL\"\n",
    "\n",
    "# ç¡®ä¿å½“å‰å·¥ä½œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(\"Current working dir:\", os.getcwd())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a5f24-4203-4408-b820-d9b85fcd7fd6",
   "metadata": {},
   "source": [
    "# ğŸ“¦ Cell 2ï¼šå®‰è£… / å¯¼å…¥ä¾èµ–ï¼ˆå¦‚æœå·²ç»è£…å¥½å¯ä»¥è·³è¿‡å®‰è£…è¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03c585d0-d134-42d7-a03f-6242fdf5f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: å¯¼å…¥ä¾èµ–ï¼ˆå¦‚æœç¼ºåŒ…å† pip å®‰è£…ï¼‰\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "except ImportError:\n",
    "    # å¦‚æœæŠ¥é”™å°±å–æ¶ˆä¸‹ä¸€è¡Œæ³¨é‡Š\n",
    "    # !pip install transformers accelerate\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "    print(\"wandb not installed, set USE_WANDB = False later.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fbcf1-c792-41f9-84db-918e4043dd57",
   "metadata": {},
   "source": [
    "# ğŸ“š Cell 3ï¼šåŠ è½½ LogicNLI æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8987d6-4b08-4d29-a441-1b223685ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading LogicNLI from /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/data/LogicNLI/dev.json\n",
      "[INFO] Loaded 300 examples.\n",
      "Sample keys: ['id', 'facts', 'rules', 'conjecture', 'answer']\n",
      "Sample example: {'id': 'LogicNLI_1', 'facts': ['Vivian is eager.', 'Rhett is not modest.', 'Vivian is confused.', 'Vivian is not lazy.', 'Atwater is eager.', 'Philip is not confused.', 'Giles is not eager.', 'Giles is not real.', 'Kilby is not eager.', 'Ramon is eager.', 'Rhett is not sociable.', 'Rhett is lazy.'], 'rules': ['Someone being not eager is equivalent to being not sociable.', 'If someone is not lazy, then he is both sociable and modest.', 'If there is someone who is not eager, then Ramon is not lazy.', 'Philip being eager or Kilby being real implies that Philip is not lazy.', 'If there is someone who is both not sociable and not modest, then Kilby is not lazy.', 'If someone is modest or not confused, then he is not eager.', 'It can be concluded that Atwater is eager once knowing that Philip is modest and Atwater is confused.', 'If someone is sociable and real, then he is both not lazy and not modest, and vice versa.', 'If there is someone who is not eager, then Vivian is lazy and Ramon is confused.', 'If all people are eager, then Giles is not sociable and Atwater is not modest.', 'Someone is sociable if and only if he is modest.', 'Ramon being real is equivalent to Rhett being not modest and Philip being lazy.'], 'conjecture': 'Vivian is confused.', 'answer': 'A'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: åŠ è½½ LogicNLI æ•°æ®é›†\n",
    "\n",
    "import os\n",
    "\n",
    "def load_json_or_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    if path.endswith(\".jsonl\"):\n",
    "        data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                data.append(json.loads(line))\n",
    "        return data\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "            return obj[\"data\"]\n",
    "        return [obj]\n",
    "    raise ValueError(f\"Unsupported JSON format: {path}\")\n",
    "\n",
    "\n",
    "def load_logicnli(split: str = \"dev\") -> List[Dict[str, Any]]:\n",
    "    path = os.path.join(PROJECT_ROOT, \"data\", \"LogicNLI\", f\"{split}.json\")\n",
    "    print(f\"[INFO] Loading LogicNLI from {path}\")\n",
    "    data = load_json_or_jsonl(path)\n",
    "    print(f\"[INFO] Loaded {len(data)} examples.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "logicnli_data = load_logicnli(\"dev\")\n",
    "\n",
    "# ç®€å•çœ‹ä¸€æ¡æ ·æœ¬çš„å­—æ®µï¼Œæ–¹ä¾¿ä½ ç¡®è®¤æ ¼å¼\n",
    "print(\"Sample keys:\", list(logicnli_data[0].keys()))\n",
    "print(\"Sample example:\", logicnli_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02739568-3885-41df-9229-7643b529bc6a",
   "metadata": {},
   "source": [
    "# âœï¸ Cell 4ï¼šæ„é€ è¾“å…¥æ–‡æœ¬ï¼ˆLogicNLI â†’ LLM promptï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "340075c9-b01f-46db-aebf-2dede2b7d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a logical reasoner. Given the premise and hypothesis, you analyze their logical relation.\n",
      "\n",
      "Premise: \n",
      "Hypothesis: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: æ„é€  LogicNLI çš„ LLM è¾“å…¥æ–‡æœ¬\n",
    "# æŒ‰å¸¸è§ LogicNLI æ ¼å¼å†™äº†ä¸€ä¸ªç‰ˆæœ¬ï¼Œä½ å¯ä»¥æ ¹æ®ä¸Šä¸€ cell æ‰“å°çš„å­—æ®µå¾®è°ƒ\n",
    "\n",
    "def build_input_text_logicnli(ex: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    å‡è®¾ LogicNLI æ¯æ¡æ ·æœ¬å¤§æ¦‚æœ‰ï¼š\n",
    "        - premise / context\n",
    "        - hypothesis / query\n",
    "        - label (entailment / contradiction / neutral) ä¹‹ç±»\n",
    "    ä½ å¯ä»¥æ ¹æ®å®é™…å­—æ®µååšæ›¿æ¢ã€‚\n",
    "    \"\"\"\n",
    "    premise = ex.get(\"premise\", ex.get(\"context\", \"\"))\n",
    "    hypothesis = ex.get(\"hypothesis\", ex.get(\"query\", \"\"))\n",
    "\n",
    "    # ä¹Ÿå¯ä»¥æŠŠ label ä¿¡æ¯å»æ‰ï¼Œé¿å…æ³„æ¼ç­”æ¡ˆ\n",
    "    # label = ex.get(\"label\", \"\")\n",
    "\n",
    "    text = (\n",
    "        \"You are a logical reasoner. \"\n",
    "        \"Given the premise and hypothesis, you analyze their logical relation.\\n\\n\"\n",
    "        f\"Premise: {premise}\\n\"\n",
    "        f\"Hypothesis: {hypothesis}\\n\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "# ç®€å•æµ‹è¯•æ„é€ çš„ prompt\n",
    "print(build_input_text_logicnli(logicnli_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768e4f1-233d-420c-bce7-7ae5d4eadcab",
   "metadata": {},
   "source": [
    "# ğŸ§  Cell 5ï¼šåœ¨ notebook é‡Œå®šä¹‰ LLaMA-3 åç«¯ï¼ˆHFTraceCausalLMï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbe77117-81bf-4036-9043-a48e0eca0a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "[INFO] Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 with dtype=bfloat16\n",
      "num hidden states (layers+embedding): 23\n",
      "tokens of first example (truncated): ['<s>', 'â–You', 'â–are', 'â–a', 'â–logical', 'â–reason', 'er', '.', 'â–Given', 'â–the', 'â–prem', 'ise', 'â–and', 'â–hypothesis', ',', 'â–you', 'â–analyze', 'â–their', 'â–logical', 'â–relation']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: å®šä¹‰ HFTraceCausalLMï¼ˆå»æ‰ device_mapï¼Œä¸ä¾èµ– accelerateï¼‰\n",
    "\n",
    "class HFTraceCausalLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # å…ˆç”¨ TinyLlama debug\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"bfloat16\",\n",
    "        max_length: int = 1024,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(f\"[INFO] Loading tokenizer: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # dtype æ˜ å°„\n",
    "        dtype_map = {\n",
    "            \"float32\": torch.float32,\n",
    "            \"float16\": torch.float16,\n",
    "            \"bfloat16\": torch.bfloat16,\n",
    "        }\n",
    "        torch_dtype = dtype_map[dtype]\n",
    "\n",
    "        print(f\"[INFO] Loading model: {model_name} with dtype={dtype}\")\n",
    "        # å…³é”®æ”¹åŠ¨ï¼šä¸å†ä¼  device_mapï¼Œè¿™æ ·å°±ä¸éœ€è¦ accelerate\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "        # æ‰‹åŠ¨æŠŠæ¨¡å‹æ¬åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆè¿™é‡Œå°±æ˜¯ GPU6ï¼Œå› ä¸ºä½ è®¾äº† CUDA_VISIBLE_DEVICES=6ï¼‰\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, prompts: List[str]) -> Dict[str, Any]:\n",
    "        batch_enc = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        # æŠŠè¾“å…¥æ¬åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡\n",
    "        batch_enc = {k: v.to(self.device) for k, v in batch_enc.items()}\n",
    "\n",
    "        outputs = self.model(\n",
    "            **batch_enc,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states  # tuple(num_layers+1, B, T, D)\n",
    "        # è¿™é‡Œç›´æ¥åœ¨ CPU ä¸Šè½¬æˆ float32ï¼Œåé¢ä»»ä½•åœ°æ–¹ç”¨åˆ° hidden_states éƒ½ä¸ä¼šå†é‡åˆ° bfloat16 çš„é—®é¢˜\n",
    "        hs_list = [h.detach().to(torch.float32).cpu() for h in hidden_states]\n",
    "\n",
    "        tokens = [\n",
    "            self.tokenizer.convert_ids_to_tokens(ids.tolist())\n",
    "            for ids in batch_enc[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"hidden_states\": hs_list,\n",
    "            \"tokens\": tokens,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"meta\": {\n",
    "                \"num_layers\": len(hs_list) - 1,\n",
    "                \"seq_lens\": batch_enc[\"attention_mask\"].sum(dim=-1).tolist(),\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# æµ‹è¯•ä¸€ä¸‹æ¨¡å‹èƒ½å¦æ­£å¸¸ forward ä¸€å°æ¡\n",
    "llama_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # å…ˆç”¨è¿™ä¸ªï¼Œä¹‹åå†æ¢å› LLaMA-3\n",
    "hf_model = HFTraceCausalLM(\n",
    "    model_name=llama_model_name,\n",
    "    device=\"cuda\",         # å¯¹åº” CUDA_VISIBLE_DEVICES=6\n",
    "    dtype=\"bfloat16\",\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "test_prompt = build_input_text_logicnli(logicnli_data[0])\n",
    "test_outputs = hf_model.encode([test_prompt])\n",
    "\n",
    "print(\"num hidden states (layers+embedding):\", len(test_outputs[\"hidden_states\"]))\n",
    "print(\"tokens of first example (truncated):\", test_outputs[\"tokens\"][0][:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c19027-5f34-407c-9951-bb1c23154165",
   "metadata": {},
   "source": [
    "# ğŸ’¾ Cell 6ï¼šä¿å­˜æ¯å±‚ embedding çš„å·¥å…·å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a316d5f-4695-448c-9858-8ff3abaf3d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding will be saved under: /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: ä¿å­˜æ¯å±‚ embedding çš„å·¥å…·å‡½æ•°\n",
    "\n",
    "def save_embeddings(\n",
    "    per_layer_vecs: List[np.ndarray],\n",
    "    ex_id: str,\n",
    "    model_name: str,\n",
    "    dataset: str,\n",
    "    save_root: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    per_layer_vecs: List[np.ndarray]ï¼Œæ¯ä¸ª (D,) æˆ– (T, D)\n",
    "    ex_id: æ ·æœ¬ idï¼ˆstringï¼‰\n",
    "    \"\"\"\n",
    "    model_short = model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "    save_dir = os.path.join(save_root, dataset, model_short)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"ex_{ex_id}.npz\")\n",
    "    np.savez_compressed(save_path, *per_layer_vecs)\n",
    "    return save_path\n",
    "\n",
    "\n",
    "EMBED_SAVE_ROOT = os.path.join(PROJECT_ROOT, \"results\", \"embeddings\")\n",
    "print(\"Embedding will be saved under:\", EMBED_SAVE_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4b42c-8f35-4481-b815-711f47ba1d0d",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Cell 7ï¼šé…ç½® wandbï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcaccfeb-161d-461e-8c5b-a7b7ac204f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing wandb...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LogicNLI_dev_TinyLlama-1.1B-Chat-v1.0_layerwise</strong> at: <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/fsgvs6gv' target=\"_blank\">https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/fsgvs6gv</a><br> View project at: <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL' target=\"_blank\">https://wandb.ai/chengjiezheng-umb/TRACE_26ACL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251203_063744-fsgvs6gv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/wandb/run-20251203_064356-59ipj854</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/59ipj854' target=\"_blank\">LogicNLI_dev_TinyLlama-1.1B-Chat-v1.0_layerwise</a></strong> to <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL' target=\"_blank\">https://wandb.ai/chengjiezheng-umb/TRACE_26ACL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/59ipj854' target=\"_blank\">https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/59ipj854</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: é…ç½® wandbï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "USE_WANDB = True and WANDB_AVAILABLE  # æƒ³å…³æ‰å°±æ”¹æˆ False\n",
    "\n",
    "WANDB_PROJECT = \"TRACE_26ACL\"\n",
    "WANDB_ENTITY = None  # å¦‚æœä½ æœ‰ entityï¼Œå°±å¡«ä½ çš„ entity å­—ç¬¦ä¸²ï¼›å¦åˆ™ None\n",
    "\n",
    "run_name = f\"LogicNLI_dev_{llama_model_name.split('/')[-1]}_layerwise\"\n",
    "\n",
    "if USE_WANDB:\n",
    "    print(\"[INFO] Initializing wandb...\")\n",
    "    wandb.login()  # å¦‚æœä¹‹å‰æ²¡ç™»å½•è¿‡ï¼Œä¼šæç¤ºä½ ç²˜ token\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"dataset\": \"LogicNLI\",\n",
    "            \"split\": \"dev\",\n",
    "            \"model_name\": llama_model_name,\n",
    "            \"max_length\": 512,\n",
    "            \"note\": \"layerwise embeddings from notebook on GPU6\",\n",
    "        },\n",
    "    )\n",
    "else:\n",
    "    print(\"[INFO] wandb disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff023b-d1b3-4cba-b37a-c264c06a005a",
   "metadata": {},
   "source": [
    "# ğŸš€ Cell 8ï¼šä¸»å¾ªç¯â€”â€”æŠ½å–æ¯å±‚ embedding & ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86964c63-af58-4958-977a-19c0d60a7dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total examples to process: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   1%|â–ˆâ–ˆâ–                                                                                                                                                                | 1/75 [00:00<00:11,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model returned 23 hidden states (incl. embedding layer).\n",
      "Example 1 saved to /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0/ex_LogicNLI_1.npz\n",
      "Example 2 saved to /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0/ex_LogicNLI_2.npz\n",
      "Example 3 saved to /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0/ex_LogicNLI_3.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:06<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Done. Total processed examples: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>layer_0_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_10_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_11_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_12_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_13_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_14_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_15_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_16_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_17_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>layer_18_avg_l2</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>+14</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>layer_0_avg_l2</td><td>0.21073</td></tr><tr><td>layer_10_avg_l2</td><td>3.92483</td></tr><tr><td>layer_11_avg_l2</td><td>4.52346</td></tr><tr><td>layer_12_avg_l2</td><td>5.41406</td></tr><tr><td>layer_13_avg_l2</td><td>5.65207</td></tr><tr><td>layer_14_avg_l2</td><td>6.41559</td></tr><tr><td>layer_15_avg_l2</td><td>7.09263</td></tr><tr><td>layer_16_avg_l2</td><td>8.42116</td></tr><tr><td>layer_17_avg_l2</td><td>11.43604</td></tr><tr><td>layer_18_avg_l2</td><td>13.4553</td></tr><tr><td>+14</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LogicNLI_dev_TinyLlama-1.1B-Chat-v1.0_layerwise</strong> at: <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/59ipj854' target=\"_blank\">https://wandb.ai/chengjiezheng-umb/TRACE_26ACL/runs/59ipj854</a><br> View project at: <a href='https://wandb.ai/chengjiezheng-umb/TRACE_26ACL' target=\"_blank\">https://wandb.ai/chengjiezheng-umb/TRACE_26ACL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251203_064356-59ipj854/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 8: ä¸»å¾ªç¯â€”â€”æŠ½å–æ¯å±‚ embedding & ä¿å­˜\n",
    "\n",
    "BATCH_SIZE = 4         # å¯ä»¥æ ¹æ® GPU æ˜¾å­˜æ”¹å¤§/æ”¹å°\n",
    "MAX_SAMPLES = -1       # >0 åˆ™åªè·‘å‰ N æ¡æ ·æœ¬ï¼Œä¾¿äº debug\n",
    "DATASET_NAME = \"LogicNLI\"\n",
    "SPLIT = \"dev\"\n",
    "\n",
    "data = logicnli_data\n",
    "if MAX_SAMPLES > 0:\n",
    "    data = data[:MAX_SAMPLES]\n",
    "    print(f\"[INFO] Using first {len(data)} samples (MAX_SAMPLES={MAX_SAMPLES}).\")\n",
    "\n",
    "num_examples = len(data)\n",
    "print(f\"[INFO] Total examples to process: {num_examples}\")\n",
    "\n",
    "layer_norm_sums = None\n",
    "layer_norm_counts = 0\n",
    "num_layers_plus_one = None\n",
    "\n",
    "global_idx = 0\n",
    "\n",
    "for start in tqdm(range(0, num_examples, BATCH_SIZE), desc=\"Batches\"):\n",
    "    end = min(start + BATCH_SIZE, num_examples)\n",
    "    batch = data[start:end]\n",
    "\n",
    "    prompts = [build_input_text_logicnli(ex) for ex in batch]\n",
    "\n",
    "    outputs = hf_model.encode(prompts)\n",
    "    hidden_states = outputs[\"hidden_states\"]   # list[L+1]ï¼Œæ¯ä¸ª (B, T, D)\n",
    "    model_name = outputs.get(\"model_name\", llama_model_name)\n",
    "\n",
    "    if num_layers_plus_one is None:\n",
    "        num_layers_plus_one = len(hidden_states)\n",
    "        print(f\"[INFO] Model returned {num_layers_plus_one} hidden states (incl. embedding layer).\")\n",
    "\n",
    "    for b, ex in enumerate(batch):\n",
    "        ex_id = str(ex.get(\"id\", global_idx))   # å¦‚æœæ ·æœ¬æ²¡æœ‰ id å­—æ®µï¼Œå°±ç”¨ index\n",
    "\n",
    "        per_layer_vecs = []\n",
    "        layer_norms = []\n",
    "\n",
    "        for layer_idx, h in enumerate(hidden_states):\n",
    "            # h: (B, T, D) torch.Tensor\n",
    "            seq_hidden = h[b]          # (T, D)\n",
    "\n",
    "            # è¿™é‡Œæˆ‘ä»¬å–æœ€åä¸€ä¸ª token çš„å‘é‡ä½œä¸ºè¯¥å±‚çš„ embedding\n",
    "            last_vec = seq_hidden[-1]  # (D,)\n",
    "            vec_np = last_vec.detach().cpu().numpy()\n",
    "            per_layer_vecs.append(vec_np)\n",
    "\n",
    "            layer_norms.append(float(last_vec.norm(p=2).item()))\n",
    "\n",
    "        save_path = save_embeddings(\n",
    "            per_layer_vecs=per_layer_vecs,\n",
    "            ex_id=ex_id,\n",
    "            model_name=model_name,\n",
    "            dataset=DATASET_NAME,\n",
    "            save_root=EMBED_SAVE_ROOT,\n",
    "        )\n",
    "\n",
    "        global_idx += 1\n",
    "\n",
    "        if global_idx <= 3:\n",
    "            print(f\"Example {global_idx} saved to {save_path}\")\n",
    "\n",
    "        # æ›´æ–°å…¨å±€ç»Ÿè®¡\n",
    "        if layer_norm_sums is None:\n",
    "            layer_norm_sums = np.zeros(len(layer_norms), dtype=np.float64)\n",
    "        layer_norm_sums += np.array(layer_norms, dtype=np.float64)\n",
    "        layer_norm_counts += 1\n",
    "\n",
    "    # æ¯ä¸ª batch ä¹‹åï¼Œç”¨ wandb è®°å½•ä¸€ä¸‹å„å±‚å¹³å‡ L2 èŒƒæ•°\n",
    "    if USE_WANDB:\n",
    "        avg_layer_norms = layer_norm_sums / max(layer_norm_counts, 1)\n",
    "        log_dict = {\n",
    "            f\"layer_{i}_avg_l2\": float(v)\n",
    "            for i, v in enumerate(avg_layer_norms)\n",
    "        }\n",
    "        log_dict[\"num_processed\"] = layer_norm_counts\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "print(f\"[INFO] Done. Total processed examples: {layer_norm_counts}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0aeaee-34bb-4b8b-bdfc-471ef1384a3b",
   "metadata": {},
   "source": [
    "# ğŸ” Cell 9ï¼šç®€å•æ£€æŸ¥ä¿å­˜çš„ embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5bba710-b002-41d2-bb5a-d44ee670861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dir: /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0\n",
      "Num files: 300\n",
      "First 3 files: ['/beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0/ex_LogicNLI_1.npz', '/beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0/ex_LogicNLI_10.npz', '/beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/results/embeddings/LogicNLI/TinyLlama-1.1B-Chat-v1.0/ex_LogicNLI_100.npz']\n",
      "Num layers stored: 23\n",
      "layer 0: shape (2048,)\n",
      "layer 1: shape (2048,)\n",
      "layer 2: shape (2048,)\n",
      "layer 3: shape (2048,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: æ£€æŸ¥ä¸€ä¸‹ä¿å­˜çš„ embedding æ–‡ä»¶ç»“æ„\n",
    "\n",
    "import glob\n",
    "\n",
    "model_short = llama_model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "embed_dir = os.path.join(EMBED_SAVE_ROOT, DATASET_NAME, model_short)\n",
    "print(\"Embedding dir:\", embed_dir)\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(embed_dir, \"ex_*.npz\")))\n",
    "print(\"Num files:\", len(files))\n",
    "print(\"First 3 files:\", files[:3])\n",
    "\n",
    "# çœ‹ä¸€çœ¼ä¸€ä¸ª .npz é‡Œæœ‰å‡ å±‚ï¼Œä»¥åŠå‘é‡ç»´åº¦\n",
    "if files:\n",
    "    sample_npz = np.load(files[0])\n",
    "    print(\"Num layers stored:\", len(sample_npz.files))\n",
    "    for i, key in enumerate(sample_npz.files):\n",
    "        arr = sample_npz[key]\n",
    "        print(f\"layer {i}: shape {arr.shape}\")\n",
    "        if i >= 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c144e91-a223-4a06-99a0-9e710c5150fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
