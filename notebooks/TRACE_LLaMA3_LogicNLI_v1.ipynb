{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f495b5a-c432-42a0-8041-9f2778f2afc8",
   "metadata": {},
   "source": [
    "# ğŸ§± Cell 1ï¼šç¯å¢ƒä¸è·¯å¾„è®¾ç½®ï¼ˆGPU6ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc4d8d1-de32-4c60-8642-07aed758acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL\n",
      "CUDA_VISIBLE_DEVICES: 6\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: ç¯å¢ƒä¸è·¯å¾„è®¾ç½®ï¼ˆGPU6ï¼‰\n",
    "\n",
    "import os\n",
    "\n",
    "# åªä½¿ç”¨ GPU6\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ï¼ˆæŒ‰ä½ å®é™…è·¯å¾„æ¥ï¼‰\n",
    "PROJECT_ROOT = \"/beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL\"\n",
    "\n",
    "# ç¡®ä¿å½“å‰å·¥ä½œç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(\"Current working dir:\", os.getcwd())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a5f24-4203-4408-b820-d9b85fcd7fd6",
   "metadata": {},
   "source": [
    "# ğŸ“¦ Cell 2ï¼šå®‰è£… / å¯¼å…¥ä¾èµ–ï¼ˆå¦‚æœå·²ç»è£…å¥½å¯ä»¥è·³è¿‡å®‰è£…è¡Œï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c585d0-d134-42d7-a03f-6242fdf5f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: å¯¼å…¥ä¾èµ–ï¼ˆå¦‚æœç¼ºåŒ…å† pip å®‰è£…ï¼‰\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "except ImportError:\n",
    "    # å¦‚æœæŠ¥é”™å°±å–æ¶ˆä¸‹ä¸€è¡Œæ³¨é‡Š\n",
    "    # !pip install transformers accelerate\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "    print(\"wandb not installed, set USE_WANDB = False later.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fbcf1-c792-41f9-84db-918e4043dd57",
   "metadata": {},
   "source": [
    "# ğŸ“š Cell 3ï¼šåŠ è½½ LogicNLI æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8987d6-4b08-4d29-a441-1b223685ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading LogicNLI from /beacon/data01/chengjie.zheng001/Projects/07TRACE_LLMs/26ACL/TRACE_26ACL/data/LogicNLI/dev.json\n",
      "[INFO] Loaded 300 examples.\n",
      "Sample keys: ['id', 'facts', 'rules', 'conjecture', 'answer']\n",
      "Sample example: {'id': 'LogicNLI_1', 'facts': ['Vivian is eager.', 'Rhett is not modest.', 'Vivian is confused.', 'Vivian is not lazy.', 'Atwater is eager.', 'Philip is not confused.', 'Giles is not eager.', 'Giles is not real.', 'Kilby is not eager.', 'Ramon is eager.', 'Rhett is not sociable.', 'Rhett is lazy.'], 'rules': ['Someone being not eager is equivalent to being not sociable.', 'If someone is not lazy, then he is both sociable and modest.', 'If there is someone who is not eager, then Ramon is not lazy.', 'Philip being eager or Kilby being real implies that Philip is not lazy.', 'If there is someone who is both not sociable and not modest, then Kilby is not lazy.', 'If someone is modest or not confused, then he is not eager.', 'It can be concluded that Atwater is eager once knowing that Philip is modest and Atwater is confused.', 'If someone is sociable and real, then he is both not lazy and not modest, and vice versa.', 'If there is someone who is not eager, then Vivian is lazy and Ramon is confused.', 'If all people are eager, then Giles is not sociable and Atwater is not modest.', 'Someone is sociable if and only if he is modest.', 'Ramon being real is equivalent to Rhett being not modest and Philip being lazy.'], 'conjecture': 'Vivian is confused.', 'answer': 'A'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: åŠ è½½ LogicNLI æ•°æ®é›†\n",
    "\n",
    "import os\n",
    "\n",
    "def load_json_or_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    if path.endswith(\".jsonl\"):\n",
    "        data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                data.append(json.loads(line))\n",
    "        return data\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        if \"data\" in obj and isinstance(obj[\"data\"], list):\n",
    "            return obj[\"data\"]\n",
    "        return [obj]\n",
    "    raise ValueError(f\"Unsupported JSON format: {path}\")\n",
    "\n",
    "\n",
    "def load_logicnli(split: str = \"dev\") -> List[Dict[str, Any]]:\n",
    "    path = os.path.join(PROJECT_ROOT, \"data\", \"LogicNLI\", f\"{split}.json\")\n",
    "    print(f\"[INFO] Loading LogicNLI from {path}\")\n",
    "    data = load_json_or_jsonl(path)\n",
    "    print(f\"[INFO] Loaded {len(data)} examples.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "logicnli_data = load_logicnli(\"dev\")\n",
    "\n",
    "# ç®€å•çœ‹ä¸€æ¡æ ·æœ¬çš„å­—æ®µï¼Œæ–¹ä¾¿ä½ ç¡®è®¤æ ¼å¼\n",
    "print(\"Sample keys:\", list(logicnli_data[0].keys()))\n",
    "print(\"Sample example:\", logicnli_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02739568-3885-41df-9229-7643b529bc6a",
   "metadata": {},
   "source": [
    "# âœï¸ Cell 4ï¼šæ„é€ è¾“å…¥æ–‡æœ¬ï¼ˆLogicNLI â†’ LLM promptï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "340075c9-b01f-46db-aebf-2dede2b7d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a logical reasoner. Given the premise and hypothesis, you analyze their logical relation.\n",
      "\n",
      "Premise: \n",
      "Hypothesis: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: æ„é€  LogicNLI çš„ LLM è¾“å…¥æ–‡æœ¬\n",
    "# æŒ‰å¸¸è§ LogicNLI æ ¼å¼å†™äº†ä¸€ä¸ªç‰ˆæœ¬ï¼Œä½ å¯ä»¥æ ¹æ®ä¸Šä¸€ cell æ‰“å°çš„å­—æ®µå¾®è°ƒ\n",
    "\n",
    "def build_input_text_logicnli(ex: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    å‡è®¾ LogicNLI æ¯æ¡æ ·æœ¬å¤§æ¦‚æœ‰ï¼š\n",
    "        - premise / context\n",
    "        - hypothesis / query\n",
    "        - label (entailment / contradiction / neutral) ä¹‹ç±»\n",
    "    ä½ å¯ä»¥æ ¹æ®å®é™…å­—æ®µååšæ›¿æ¢ã€‚\n",
    "    \"\"\"\n",
    "    premise = ex.get(\"premise\", ex.get(\"context\", \"\"))\n",
    "    hypothesis = ex.get(\"hypothesis\", ex.get(\"query\", \"\"))\n",
    "\n",
    "    # ä¹Ÿå¯ä»¥æŠŠ label ä¿¡æ¯å»æ‰ï¼Œé¿å…æ³„æ¼ç­”æ¡ˆ\n",
    "    # label = ex.get(\"label\", \"\")\n",
    "\n",
    "    text = (\n",
    "        \"You are a logical reasoner. \"\n",
    "        \"Given the premise and hypothesis, you analyze their logical relation.\\n\\n\"\n",
    "        f\"Premise: {premise}\\n\"\n",
    "        f\"Hypothesis: {hypothesis}\\n\"\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "# ç®€å•æµ‹è¯•æ„é€ çš„ prompt\n",
    "print(build_input_text_logicnli(logicnli_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768e4f1-233d-420c-bce7-7ae5d4eadcab",
   "metadata": {},
   "source": [
    "# ğŸ§  Cell 5ï¼šåœ¨ notebook é‡Œå®šä¹‰ LLaMA-3 åç«¯ï¼ˆHFTraceCausalLMï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe77117-81bf-4036-9043-a48e0eca0a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading tokenizer: meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengjie.zheng001/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n403 Client Error. (Request ID: Root=1-692fd5e2-0f6f260e7cea54a35c27ac09;983b2860-28f9-4908-bc66-483ceac4183c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1658\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1655\u001b[0m ):\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1546\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1463\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[0;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:426\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-692fd5e2-0f6f260e7cea54a35c27ac09;983b2860-28f9-4908-bc66-483ceac4183c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# æµ‹è¯•ä¸€ä¸‹æ¨¡å‹èƒ½å¦æ­£å¸¸ forward ä¸€å°æ¡\u001b[39;00m\n\u001b[1;32m     76\u001b[0m llama_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# å¦‚éœ€åˆ«çš„ LLaMA3 variant å¯ä»¥æ”¹\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m hf_model \u001b[38;5;241m=\u001b[39m \u001b[43mHFTraceCausalLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# å¯¹åº” CUDA_VISIBLE_DEVICES=6\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m test_prompt \u001b[38;5;241m=\u001b[39m build_input_text_logicnli(logicnli_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     85\u001b[0m test_outputs \u001b[38;5;241m=\u001b[39m hf_model\u001b[38;5;241m.\u001b[39mencode([test_prompt])\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mHFTraceCausalLM.__init__\u001b[0;34m(self, model_name, device, dtype, max_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Loading tokenizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:837\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:934\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    932\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 934\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    936\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n403 Client Error. (Request ID: Root=1-692fd5e2-0f6f260e7cea54a35c27ac09;983b2860-28f9-4908-bc66-483ceac4183c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to ask for access."
     ]
    }
   ],
   "source": [
    "# Cell 5: å®šä¹‰ HFTraceCausalLMï¼ˆLLaMA-3 åç«¯ï¼‰\n",
    "\n",
    "class HFTraceCausalLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"bfloat16\",\n",
    "        max_length: int = 1024,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(f\"[INFO] Loading tokenizer: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # dtype æ˜ å°„\n",
    "        dtype_map = {\n",
    "            \"float32\": torch.float32,\n",
    "            \"float16\": torch.float16,\n",
    "            \"bfloat16\": torch.bfloat16,\n",
    "        }\n",
    "        torch_dtype = dtype_map[dtype]\n",
    "\n",
    "        print(f\"[INFO] Loading model: {model_name} with dtype={dtype} on device={device}\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=device,   # å› ä¸ºæˆ‘ä»¬è®¾äº† CUDA_VISIBLE_DEVICES=6ï¼Œè¿™é‡Œ \"cuda\" æˆ– \"auto\" éƒ½ä¼šç”¨é‚£å—å¡\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, prompts: List[str]) -> Dict[str, Any]:\n",
    "        batch_enc = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        # æŠŠè¾“å…¥æ¬åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼ˆä¸€èˆ¬å°±æ˜¯ cuda:0ï¼Œå¯¹åº” GPU6ï¼‰\n",
    "        batch_enc = {k: v.to(self.model.device) for k, v in batch_enc.items()}\n",
    "\n",
    "        outputs = self.model(\n",
    "            **batch_enc,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states  # tuple(num_layers+1, B, T, D)\n",
    "        hs_list = [h.detach().cpu() for h in hidden_states]\n",
    "\n",
    "        tokens = [\n",
    "            self.tokenizer.convert_ids_to_tokens(ids.tolist())\n",
    "            for ids in batch_enc[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"hidden_states\": hs_list,\n",
    "            \"tokens\": tokens,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"meta\": {\n",
    "                \"num_layers\": len(hs_list) - 1,\n",
    "                \"seq_lens\": batch_enc[\"attention_mask\"].sum(dim=-1).tolist(),\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# æµ‹è¯•ä¸€ä¸‹æ¨¡å‹èƒ½å¦æ­£å¸¸ forward ä¸€å°æ¡\n",
    "llama_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # å¦‚éœ€åˆ«çš„ LLaMA3 variant å¯ä»¥æ”¹\n",
    "hf_model = HFTraceCausalLM(\n",
    "    model_name=llama_model_name,\n",
    "    device=\"cuda\",         # å¯¹åº” CUDA_VISIBLE_DEVICES=6\n",
    "    dtype=\"bfloat16\",\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "test_prompt = build_input_text_logicnli(logicnli_data[0])\n",
    "test_outputs = hf_model.encode([test_prompt])\n",
    "\n",
    "print(\"num hidden states (layers+embedding):\", len(test_outputs[\"hidden_states\"]))\n",
    "print(\"tokens of first example (truncated):\", test_outputs[\"tokens\"][0][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c19027-5f34-407c-9951-bb1c23154165",
   "metadata": {},
   "source": [
    "# ğŸ’¾ Cell 6ï¼šä¿å­˜æ¯å±‚ embedding çš„å·¥å…·å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a316d5f-4695-448c-9858-8ff3abaf3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ä¿å­˜æ¯å±‚ embedding çš„å·¥å…·å‡½æ•°\n",
    "\n",
    "def save_embeddings(\n",
    "    per_layer_vecs: List[np.ndarray],\n",
    "    ex_id: str,\n",
    "    model_name: str,\n",
    "    dataset: str,\n",
    "    save_root: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    per_layer_vecs: List[np.ndarray]ï¼Œæ¯ä¸ª (D,) æˆ– (T, D)\n",
    "    ex_id: æ ·æœ¬ idï¼ˆstringï¼‰\n",
    "    \"\"\"\n",
    "    model_short = model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "    save_dir = os.path.join(save_root, dataset, model_short)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"ex_{ex_id}.npz\")\n",
    "    np.savez_compressed(save_path, *per_layer_vecs)\n",
    "    return save_path\n",
    "\n",
    "\n",
    "EMBED_SAVE_ROOT = os.path.join(PROJECT_ROOT, \"results\", \"embeddings\")\n",
    "print(\"Embedding will be saved under:\", EMBED_SAVE_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4b42c-8f35-4481-b815-711f47ba1d0d",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Cell 7ï¼šé…ç½® wandbï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaccfeb-161d-461e-8c5b-a7b7ac204f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: é…ç½® wandbï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "USE_WANDB = True and WANDB_AVAILABLE  # æƒ³å…³æ‰å°±æ”¹æˆ False\n",
    "\n",
    "WANDB_PROJECT = \"TRACE_26ACL\"\n",
    "WANDB_ENTITY = None  # å¦‚æœä½ æœ‰ entityï¼Œå°±å¡«ä½ çš„ entity å­—ç¬¦ä¸²ï¼›å¦åˆ™ None\n",
    "\n",
    "run_name = f\"LogicNLI_dev_{llama_model_name.split('/')[-1]}_layerwise\"\n",
    "\n",
    "if USE_WANDB:\n",
    "    print(\"[INFO] Initializing wandb...\")\n",
    "    wandb.login()  # å¦‚æœä¹‹å‰æ²¡ç™»å½•è¿‡ï¼Œä¼šæç¤ºä½ ç²˜ token\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"dataset\": \"LogicNLI\",\n",
    "            \"split\": \"dev\",\n",
    "            \"model_name\": llama_model_name,\n",
    "            \"max_length\": 512,\n",
    "            \"note\": \"layerwise embeddings from notebook on GPU6\",\n",
    "        },\n",
    "    )\n",
    "else:\n",
    "    print(\"[INFO] wandb disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff023b-d1b3-4cba-b37a-c264c06a005a",
   "metadata": {},
   "source": [
    "# ğŸš€ Cell 8ï¼šä¸»å¾ªç¯â€”â€”æŠ½å–æ¯å±‚ embedding & ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86964c63-af58-4958-977a-19c0d60a7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: ä¸»å¾ªç¯â€”â€”æŠ½å–æ¯å±‚ embedding & ä¿å­˜\n",
    "\n",
    "BATCH_SIZE = 4         # å¯ä»¥æ ¹æ® GPU æ˜¾å­˜æ”¹å¤§/æ”¹å°\n",
    "MAX_SAMPLES = -1       # >0 åˆ™åªè·‘å‰ N æ¡æ ·æœ¬ï¼Œä¾¿äº debug\n",
    "DATASET_NAME = \"LogicNLI\"\n",
    "SPLIT = \"dev\"\n",
    "\n",
    "data = logicnli_data\n",
    "if MAX_SAMPLES > 0:\n",
    "    data = data[:MAX_SAMPLES]\n",
    "    print(f\"[INFO] Using first {len(data)} samples (MAX_SAMPLES={MAX_SAMPLES}).\")\n",
    "\n",
    "num_examples = len(data)\n",
    "print(f\"[INFO] Total examples to process: {num_examples}\")\n",
    "\n",
    "layer_norm_sums = None\n",
    "layer_norm_counts = 0\n",
    "num_layers_plus_one = None\n",
    "\n",
    "global_idx = 0\n",
    "\n",
    "for start in tqdm(range(0, num_examples, BATCH_SIZE), desc=\"Batches\"):\n",
    "    end = min(start + BATCH_SIZE, num_examples)\n",
    "    batch = data[start:end]\n",
    "\n",
    "    prompts = [build_input_text_logicnli(ex) for ex in batch]\n",
    "\n",
    "    outputs = hf_model.encode(prompts)\n",
    "    hidden_states = outputs[\"hidden_states\"]   # list[L+1]ï¼Œæ¯ä¸ª (B, T, D)\n",
    "    model_name = outputs.get(\"model_name\", llama_model_name)\n",
    "\n",
    "    if num_layers_plus_one is None:\n",
    "        num_layers_plus_one = len(hidden_states)\n",
    "        print(f\"[INFO] Model returned {num_layers_plus_one} hidden states (incl. embedding layer).\")\n",
    "\n",
    "    for b, ex in enumerate(batch):\n",
    "        ex_id = str(ex.get(\"id\", global_idx))   # å¦‚æœæ ·æœ¬æ²¡æœ‰ id å­—æ®µï¼Œå°±ç”¨ index\n",
    "\n",
    "        per_layer_vecs = []\n",
    "        layer_norms = []\n",
    "\n",
    "        for layer_idx, h in enumerate(hidden_states):\n",
    "            # h: (B, T, D) torch.Tensor\n",
    "            seq_hidden = h[b]          # (T, D)\n",
    "\n",
    "            # è¿™é‡Œæˆ‘ä»¬å–æœ€åä¸€ä¸ª token çš„å‘é‡ä½œä¸ºè¯¥å±‚çš„ embedding\n",
    "            last_vec = seq_hidden[-1]  # (D,)\n",
    "            vec_np = last_vec.detach().cpu().numpy()\n",
    "            per_layer_vecs.append(vec_np)\n",
    "\n",
    "            layer_norms.append(float(last_vec.norm(p=2).item()))\n",
    "\n",
    "        save_path = save_embeddings(\n",
    "            per_layer_vecs=per_layer_vecs,\n",
    "            ex_id=ex_id,\n",
    "            model_name=model_name,\n",
    "            dataset=DATASET_NAME,\n",
    "            save_root=EMBED_SAVE_ROOT,\n",
    "        )\n",
    "\n",
    "        global_idx += 1\n",
    "\n",
    "        if global_idx <= 3:\n",
    "            print(f\"Example {global_idx} saved to {save_path}\")\n",
    "\n",
    "        # æ›´æ–°å…¨å±€ç»Ÿè®¡\n",
    "        if layer_norm_sums is None:\n",
    "            layer_norm_sums = np.zeros(len(layer_norms), dtype=np.float64)\n",
    "        layer_norm_sums += np.array(layer_norms, dtype=np.float64)\n",
    "        layer_norm_counts += 1\n",
    "\n",
    "    # æ¯ä¸ª batch ä¹‹åï¼Œç”¨ wandb è®°å½•ä¸€ä¸‹å„å±‚å¹³å‡ L2 èŒƒæ•°\n",
    "    if USE_WANDB:\n",
    "        avg_layer_norms = layer_norm_sums / max(layer_norm_counts, 1)\n",
    "        log_dict = {\n",
    "            f\"layer_{i}_avg_l2\": float(v)\n",
    "            for i, v in enumerate(avg_layer_norms)\n",
    "        }\n",
    "        log_dict[\"num_processed\"] = layer_norm_counts\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "print(f\"[INFO] Done. Total processed examples: {layer_norm_counts}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0aeaee-34bb-4b8b-bdfc-471ef1384a3b",
   "metadata": {},
   "source": [
    "# ğŸ” Cell 9ï¼šç®€å•æ£€æŸ¥ä¿å­˜çš„ embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bba710-b002-41d2-bb5a-d44ee670861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: æ£€æŸ¥ä¸€ä¸‹ä¿å­˜çš„ embedding æ–‡ä»¶ç»“æ„\n",
    "\n",
    "import glob\n",
    "\n",
    "model_short = llama_model_name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "embed_dir = os.path.join(EMBED_SAVE_ROOT, DATASET_NAME, model_short)\n",
    "print(\"Embedding dir:\", embed_dir)\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(embed_dir, \"ex_*.npz\")))\n",
    "print(\"Num files:\", len(files))\n",
    "print(\"First 3 files:\", files[:3])\n",
    "\n",
    "# çœ‹ä¸€çœ¼ä¸€ä¸ª .npz é‡Œæœ‰å‡ å±‚ï¼Œä»¥åŠå‘é‡ç»´åº¦\n",
    "if files:\n",
    "    sample_npz = np.load(files[0])\n",
    "    print(\"Num layers stored:\", len(sample_npz.files))\n",
    "    for i, key in enumerate(sample_npz.files):\n",
    "        arr = sample_npz[key]\n",
    "        print(f\"layer {i}: shape {arr.shape}\")\n",
    "        if i >= 3:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
